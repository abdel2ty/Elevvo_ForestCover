{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸŒ² Forest Cover Type Classification\n",
    "> **Task 3 | Level 2 | Multi-class Classification with XGBoost**\n",
    "\n",
    "---\n",
    "### ğŸ¯ Objective\n",
    "Predict **forest cover types** from cartographic data using gradient boosting â€” a high-impact real-world environmental ML task.\n",
    "\n",
    "### ğŸ“‹ Workflow\n",
    "1. Data Loading & Overview\n",
    "2. EDA â€” Feature Distributions & Correlations\n",
    "3. Data Preprocessing\n",
    "4. Model Training: Decision Tree â†’ Random Forest â†’ XGBoost\n",
    "5. Evaluation: Accuracy, Confusion Matrix, Classification Report\n",
    "6. Feature Importance Analysis\n",
    "7. Hyperparameter Tuning (Bonus)\n",
    "8. Save Model\n",
    "\n",
    "### ğŸ“¦ Dataset\n",
    "[Covertype â€” UCI / Kaggle](https://www.kaggle.com/datasets/uciml/forest-cover-type-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ğŸ“¦ IMPORTS\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import joblib\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                              confusion_matrix, ConfusionMatrixDisplay)\n",
    "\n",
    "try:\n",
    "    from xgboost import XGBClassifier\n",
    "    XGB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    print('XGBoost not installed â€” install via: pip install xgboost')\n",
    "    XGB_AVAILABLE = False\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams.update({\n",
    "    'figure.facecolor': '#0a0e1a',\n",
    "    'axes.facecolor': '#111827',\n",
    "    'axes.edgecolor': '#374151',\n",
    "    'axes.labelcolor': '#d1d5db',\n",
    "    'xtick.color': '#9ca3af',\n",
    "    'ytick.color': '#9ca3af',\n",
    "    'text.color': '#d1d5db',\n",
    "    'grid.color': '#1f2937',\n",
    "    'grid.alpha': 0.6,\n",
    "})\n",
    "\n",
    "FOREST_PALETTE = ['#2d6a4f', '#52b788', '#95d5b2', '#74c69d', '#40916c', '#1b4332', '#081c15']\n",
    "COVER_NAMES = {\n",
    "    1: 'Spruce/Fir', 2: 'Lodgepole Pine', 3: 'Ponderosa Pine',\n",
    "    4: 'Cottonwood/Willow', 5: 'Aspen', 6: 'Douglas-fir', 7: 'Krummholz'\n",
    "}\n",
    "\n",
    "print('âœ… Libraries loaded!')\n",
    "print(f'   XGBoost available: {XGB_AVAILABLE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‚ 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Load Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "try:\n",
    "    df = pd.read_csv('/kaggle/input/forest-cover-type-dataset/covtype.csv')\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        df = pd.read_csv('/kaggle/input/forest-cover-type/covtype.data.gz',\n",
    "                         header=None, compression='gzip')\n",
    "    except:\n",
    "        # Synthetic data matching covtype schema\n",
    "        np.random.seed(42)\n",
    "        n = 15000\n",
    "        cols = ['Elevation','Aspect','Slope','Horizontal_Distance_To_Hydrology',\n",
    "                'Vertical_Distance_To_Hydrology','Horizontal_Distance_To_Roadways',\n",
    "                'Hillshade_9am','Hillshade_Noon','Hillshade_3pm',\n",
    "                'Horizontal_Distance_To_Fire_Points']\n",
    "        data = {c: np.random.randint(0, 5000, n) for c in cols}\n",
    "        for i in range(4): data[f'Wilderness_Area{i+1}'] = np.random.randint(0,2,n)\n",
    "        for i in range(40): data[f'Soil_Type{i+1}'] = np.random.randint(0,2,n)\n",
    "        df = pd.DataFrame(data)\n",
    "        df['Cover_Type'] = np.random.randint(1, 8, n)\n",
    "        print('âš ï¸  Using synthetic data')\n",
    "\n",
    "# Standardize column names if needed\n",
    "if df.columns[-1] != 'Cover_Type':\n",
    "    df.columns = ['Elevation','Aspect','Slope','H_Dist_Hydro','V_Dist_Hydro',\n",
    "                  'H_Dist_Roads','Hillshade_9am','Hillshade_Noon','Hillshade_3pm',\n",
    "                  'H_Dist_Fire'] + \\\n",
    "                 [f'Wilderness_{i}' for i in range(4)] + \\\n",
    "                 [f'Soil_{i}' for i in range(40)] + ['Cover_Type']\n",
    "\n",
    "# Sample for speed (full 580k rows are used in production)\n",
    "if len(df) > 50000:\n",
    "    df = df.sample(50000, random_state=42)\n",
    "    print(f'ğŸ“Š Sampled 50,000 rows for demonstration')\n",
    "\n",
    "print(f'ğŸ“ Shape: {df.shape}')\n",
    "print(f'ğŸ“‹ Target Distribution:')\n",
    "print(df['Cover_Type'].value_counts().sort_index().to_string())\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Target Class Distribution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "fig.suptitle('ğŸŒ² Target Class Distribution', fontsize=15, fontweight='bold')\n",
    "\n",
    "cover_counts = df['Cover_Type'].value_counts().sort_index()\n",
    "cover_labels = [COVER_NAMES.get(i, f'Type {i}') for i in cover_counts.index]\n",
    "\n",
    "bars = axes[0].bar(cover_labels, cover_counts.values,\n",
    "                   color=FOREST_PALETTE[:len(cover_counts)], edgecolor='white', alpha=0.9)\n",
    "axes[0].set_title('Cover Type Counts')\n",
    "axes[0].set_xticklabels(cover_labels, rotation=30, ha='right', fontsize=9)\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].grid(True, axis='y')\n",
    "for bar, val in zip(bars, cover_counts.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100, f'{val:,}', \n",
    "                 ha='center', fontsize=8)\n",
    "\n",
    "wedges, texts, autotexts = axes[1].pie(\n",
    "    cover_counts.values, labels=cover_labels,\n",
    "    colors=FOREST_PALETTE[:len(cover_counts)],\n",
    "    autopct='%1.1f%%', pctdistance=0.8, startangle=90,\n",
    "    wedgeprops={'edgecolor': '#0a0e1a', 'linewidth': 2}\n",
    ")\n",
    "for text in autotexts: text.set_color('white'); text.set_fontsize(9)\n",
    "for text in texts: text.set_fontsize(8)\n",
    "axes[1].set_title('Class Distribution (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('class_distribution.png', dpi=150, bbox_inches='tight', facecolor='#0a0e1a')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Key Feature Distributions by Cover Type â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "key_feats = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology']\n",
    "# Handle different column naming\n",
    "available = [f for f in key_feats if f in df.columns]\n",
    "if not available:\n",
    "    available = df.select_dtypes(include=np.number).columns[:4].tolist()\n",
    "    available = [c for c in available if c != 'Cover_Type']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('ğŸ“Š Key Feature Distributions by Cover Type', fontsize=15, fontweight='bold')\n",
    "\n",
    "for ax, feat in zip(axes.flatten(), available[:4]):\n",
    "    for ct, color in zip(sorted(df['Cover_Type'].unique()), FOREST_PALETTE):\n",
    "        vals = df[df['Cover_Type'] == ct][feat]\n",
    "        ax.hist(vals, bins=30, color=color, alpha=0.5, edgecolor='none',\n",
    "                label=COVER_NAMES.get(ct, f'Type {ct}'))\n",
    "    ax.set_title(feat.replace('_', ' '), fontsize=11)\n",
    "    ax.set_xlabel(feat)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.legend(fontsize=7)\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_distributions.png', dpi=150, bbox_inches='tight', facecolor='#0a0e1a')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ 3. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Split Features & Target â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "TARGET = 'Cover_Type'\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET].values\n",
    "\n",
    "# Encode if needed\n",
    "if y.min() == 1:\n",
    "    y_xgb = y - 1  # XGBoost needs 0-indexed\n",
    "else:\n",
    "    y_xgb = y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "_, _, y_train_xgb, y_test_xgb = train_test_split(\n",
    "    X, y_xgb, test_size=0.2, random_state=42, stratify=y_xgb\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "print(f'âœ… Train: {X_train.shape} | Test: {X_test.shape}')\n",
    "print(f'   Features: {X.shape[1]}')\n",
    "print(f'   Classes: {len(np.unique(y))} cover types')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– 4. Model Training & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Train Models â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "results = []\n",
    "\n",
    "# 1. Decision Tree (baseline)\n",
    "print('ğŸŒ¿ Training Decision Tree...')\n",
    "dt = DecisionTreeClassifier(max_depth=15, random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "dt_preds = dt.predict(X_test)\n",
    "results.append({'Model': 'Decision Tree', 'Accuracy': accuracy_score(y_test, dt_preds), 'Preds': dt_preds})\n",
    "print(f'   Accuracy: {accuracy_score(y_test, dt_preds):.4f}')\n",
    "\n",
    "# 2. Random Forest\n",
    "print('ğŸŒ³ Training Random Forest...')\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)\n",
    "results.append({'Model': 'Random Forest', 'Accuracy': accuracy_score(y_test, rf_preds), 'Preds': rf_preds})\n",
    "print(f'   Accuracy: {accuracy_score(y_test, rf_preds):.4f}')\n",
    "\n",
    "# 3. XGBoost\n",
    "if XGB_AVAILABLE:\n",
    "    print('ğŸš€ Training XGBoost...')\n",
    "    xgb = XGBClassifier(\n",
    "        n_estimators=200, max_depth=8, learning_rate=0.1,\n",
    "        subsample=0.8, colsample_bytree=0.8,\n",
    "        random_state=42, eval_metric='mlogloss',\n",
    "        use_label_encoder=False, n_jobs=-1\n",
    "    )\n",
    "    xgb.fit(X_train_sc, y_train_xgb,\n",
    "            eval_set=[(X_test_sc, y_test_xgb)], verbose=False)\n",
    "    xgb_preds_raw = xgb.predict(X_test_sc)\n",
    "    xgb_preds = xgb_preds_raw + (1 if y.min() == 1 else 0)\n",
    "    results.append({'Model': 'XGBoost', 'Accuracy': accuracy_score(y_test, xgb_preds), 'Preds': xgb_preds})\n",
    "    print(f'   Accuracy: {accuracy_score(y_test, xgb_preds):.4f}')\n",
    "\n",
    "results_df = pd.DataFrame([{'Model': r['Model'], 'Accuracy': r['Accuracy']} for r in results])\n",
    "print('\\nğŸ† Model Comparison:')\n",
    "print(results_df.sort_values('Accuracy', ascending=False).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Model Comparison + Confusion Matrix Dashboard â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "best_result = max(results, key=lambda r: r['Accuracy'])\n",
    "best_preds = best_result['Preds']\n",
    "best_name = best_result['Model']\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "fig.suptitle(f'ğŸŒ² Forest Cover Classification Dashboard â€” Best: {best_name}',\n",
    "             fontsize=15, fontweight='bold')\n",
    "\n",
    "gs = fig.add_gridspec(2, 3, hspace=0.45, wspace=0.35)\n",
    "\n",
    "# Accuracy bars\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "colors_acc = ['#52b788'] * len(results)\n",
    "colors_acc[np.argmax([r['Accuracy'] for r in results])] = '#FFD93D'\n",
    "bars = ax1.bar([r['Model'] for r in results], [r['Accuracy'] for r in results],\n",
    "               color=colors_acc, edgecolor='white', alpha=0.9)\n",
    "ax1.set_title('Model Accuracy Comparison', fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim(0.5, 1.0)\n",
    "for bar, val in zip(bars, [r['Accuracy'] for r in results]):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.003, \n",
    "             f'{val:.3f}', ha='center', fontsize=10, fontweight='bold')\n",
    "ax1.set_xticklabels([r['Model'] for r in results], rotation=15, ha='right', fontsize=9)\n",
    "ax1.grid(True, axis='y')\n",
    "\n",
    "# Per-class accuracy\n",
    "ax2 = fig.add_subplot(gs[0, 1:3])\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(y_test, best_preds, output_dict=True)\n",
    "class_labels = sorted([k for k in report.keys() if k.isdigit() or (isinstance(k, str) and k.replace('.','').isdigit())])\n",
    "f1_vals = [report[str(k)]['f1-score'] for k in class_labels]\n",
    "class_names_short = [COVER_NAMES.get(int(k), f'Type {k}')[:12] for k in class_labels]\n",
    "ax2.bar(class_names_short, f1_vals, color=FOREST_PALETTE[:len(class_labels)], edgecolor='white', alpha=0.9)\n",
    "ax2.set_title(f'F1-Score per Class â€” {best_name}', fontweight='bold')\n",
    "ax2.set_ylabel('F1-Score')\n",
    "ax2.set_xticklabels(class_names_short, rotation=30, ha='right', fontsize=9)\n",
    "ax2.set_ylim(0, 1.1)\n",
    "ax2.axhline(y=np.mean(f1_vals), color='#FFD93D', linestyle='--', linewidth=1.5, label=f'Mean: {np.mean(f1_vals):.3f}')\n",
    "ax2.legend()\n",
    "ax2.grid(True, axis='y')\n",
    "\n",
    "# Confusion Matrix\n",
    "ax3 = fig.add_subplot(gs[1, 0:2])\n",
    "cm = confusion_matrix(y_test, best_preds)\n",
    "labels_cm = [COVER_NAMES.get(int(k), f'T{k}')[:10] for k in sorted(df['Cover_Type'].unique())]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=ax3,\n",
    "            xticklabels=labels_cm, yticklabels=labels_cm,\n",
    "            linewidths=0.5, linecolor='#0a0e1a',\n",
    "            annot_kws={'size': 8})\n",
    "ax3.set_title(f'Confusion Matrix â€” {best_name}', fontweight='bold')\n",
    "ax3.set_xlabel('Predicted')\n",
    "ax3.set_ylabel('Actual')\n",
    "ax3.set_xticklabels(labels_cm, rotation=30, ha='right', fontsize=8)\n",
    "ax3.set_yticklabels(labels_cm, rotation=0, fontsize=8)\n",
    "\n",
    "# Feature importance\n",
    "ax4 = fig.add_subplot(gs[1, 2])\n",
    "if best_name == 'XGBoost' and XGB_AVAILABLE:\n",
    "    importances = xgb.feature_importances_\n",
    "elif best_name == 'Random Forest':\n",
    "    importances = rf.feature_importances_\n",
    "else:\n",
    "    importances = dt.feature_importances_\n",
    "\n",
    "feat_imp = pd.Series(importances, index=X.columns).nlargest(12)\n",
    "colors_fi = plt.cm.Greens(np.linspace(0.4, 1, len(feat_imp)))\n",
    "feat_imp.sort_values().plot(kind='barh', ax=ax4, color=colors_fi)\n",
    "ax4.set_title('Top 12 Features', fontweight='bold')\n",
    "ax4.set_xlabel('Importance')\n",
    "ax4.grid(True, axis='x')\n",
    "\n",
    "plt.savefig('forest_dashboard.png', dpi=150, bbox_inches='tight', facecolor='#0a0e1a')\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nğŸ† Best Model: {best_name}')\n",
    "print(f'   Accuracy: {best_result[\"Accuracy\"]:.4f}')\n",
    "print(f'\\nğŸ“Š Full Classification Report:')\n",
    "print(classification_report(y_test, best_preds, target_names=[COVER_NAMES.get(i,'?') for i in sorted(df['Cover_Type'].unique())]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Save Best Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "if best_name == 'XGBoost' and XGB_AVAILABLE:\n",
    "    best_model_obj = xgb\n",
    "elif best_name == 'Random Forest':\n",
    "    best_model_obj = rf\n",
    "else:\n",
    "    best_model_obj = dt\n",
    "\n",
    "joblib.dump(best_model_obj, 'forest_model.pkl')\n",
    "joblib.dump(scaler, 'forest_scaler.pkl')\n",
    "\n",
    "import json\n",
    "with open('forest_features.json', 'w') as f:\n",
    "    json.dump(list(X.columns), f)\n",
    "\n",
    "print('âœ… Model artifacts saved!')\n",
    "print(f'   Best: {best_name} | Accuracy: {best_result[\"Accuracy\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Key Insights\n",
    "\n",
    "| Finding | Insight |\n",
    "|---|---|\n",
    "| **Elevation** | Most important predictor â€” each forest type has distinct elevation zones |\n",
    "| **Wilderness Area** | Categorical variable with strong class-level signal |\n",
    "| **XGBoost Wins** | Gradient boosting outperforms tree-based baselines significantly |\n",
    "| **Spruce/Fir dominant** | Majority class â€” watch for class imbalance effects |\n",
    "| **Soil Type** | Sparse binary features â€” feature selection can improve efficiency |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
